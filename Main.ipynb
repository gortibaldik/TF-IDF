{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct comparison of methods of text comprehension\n",
    "- using [__Amazon Fine Food Reviews__](https://www.kaggle.com/snap/amazon-fine-food-reviews) dataset\n",
    "- the dataset is in form of ```.csv``` document, the interesting part being columns _Text_ and _Summary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data: 992\n"
     ]
    }
   ],
   "source": [
    "from main_ntb_backend import dataset, statsKeeper\n",
    "from text_classifiers.text_rank_classifier import keywords_review as textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset._data['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good Quality Dog Food'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset._data['Summary'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the aim is to present different methods of text comprehension, namely:\n",
    "    - ```tf-idf```\n",
    "    - ```textrank```\n",
    "    - abstractive methods based on recurrent neural networks\n",
    "    \n",
    "- the main result is the importance of text preprocessing where ```tf-idf``` fails significantly without correct preprocessing (which is shown in ```TF-IDFNotebook.ipynb```) and RNNs are able to achieve better results with preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency - Inverse Document Frequency\n",
    "- each review is being treated as a separate document\n",
    "- after cruising through all the reviews, the document frequency of a word means how many reviews contain this particular word\n",
    "- term frequency is statistics of a single review, its being calculated as $\\frac{Occurences Of Word}{Number Of Words In The Review}$\n",
    "- inverse document frequency is negative logarithm of document frequency $\\ln(\\frac{Number Of Documents}{Document Frequency Of Word + 1})$\n",
    "\n",
    "- we keep the best 5 words from the review as a measure what is the most important part of the document\n",
    "\n",
    "- all the statistics are kept in ```statsKeeper```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('better', 0.28648286272145923),\n",
       " ('product', 0.24837373829266315),\n",
       " ('vitality', 0.09399881643554253),\n",
       " ('labrador', 0.09399881643554253),\n",
       " ('appreciates', 0.09399881643554253)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statsKeeper._documents[\"0\"].sorted_tf_idfs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textrank algorithm\n",
    "- from the paper [\"TextRank: Bringing Order into Text\"](http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)\n",
    "- the algorithm based on ```PageRank```, ```TextRank``` evaluates the importance of a word in a document by the number of ```links``` to the word from the other parts of document\n",
    "- the algorithm creates a graph of words appearing in a single document and draws an edge between any two words whenever they occur in the same window (meaning they are at most ```N``` words appart)\n",
    "- then the ```PageRank``` algorithm is used on the graph\n",
    "- ```PageRank``` computes the importance of a word as the probability of 'browsing' to current node based on the number of ingoing edges and the importance of their second nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2118     1  several vitality canned dog food products\n",
      "[several vitality canned dog food products]\n",
      "0.1950     1  good quality product\n",
      "[good quality product]\n"
     ]
    }
   ],
   "source": [
    "textrank(dataset._data['cleaned_text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks\n",
    "- 3 different architectures with 2 different modes of text preprocessing are used\n",
    "- the main component of the neural network is an ```LSTM``` cell\n",
    "- in each distinct architecture, we use ```softmax``` layer as the last layer and ```sparse categorical crossentropy``` as the loss function\n",
    "\n",
    "### 1. Architecture\n",
    "- encoder-decoder architecture of 3 stacked ```LSTM``` nodes in the encoder the last state passed to the only level of ```LSTM``` cells in the decoder with the softmax activation on the outputs of each time step\n",
    "- since this is the simplest architecture, the expectation is that reviews from this architecture will be of the lowest quality\n",
    "\n",
    "### 2. Architecure\n",
    "- encoder-decoder architecture, the same encoder as before, the same decoder as before, with attention on the top of encoder and decoder\n",
    "- good quality reviews are expected from this architecture\n",
    "\n",
    "### 3. Architecture\n",
    "- encoder-decoder architecture, which, in the encoder, uses bidirectional ```LSTM``` cells where the directions are concatenated, the decoder uses unidirectional ```LSTM``` cell, wit attention on the top of encoder and decoder\n",
    "- the training is expected to take more than week (since only CPUs are used) therefore the results (which are expected to be of the best quality) will be added later\n",
    "\n",
    "### 1. Preprocessing\n",
    "- only lowercasing\n",
    "\n",
    "### 2. Preprocessing\n",
    "- lowercasing, deleting links, contraction_mapping, deleting all the special characters and short words\n",
    "\n",
    "- with the second preprocessing the results will hopefully be of better quality, because the neural network will have to learn less difficult connections between the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention mechanism\n",
    "- based on [paper](https://arxiv.org/abs/1409.0473)\n",
    "- in standard encoder-decoder architecture the encoder learns the context of the input sequence and returns it as a single fixed-size vector, therefore decoder needs to learn to translate fixed-size vector into human-readable output\n",
    "- in attention driven encoder-decoder the encoder outputs the learned context at each time-step, then the final human-readable output isn't pure output of the decoder but output of the attention model at the top of encoder-decoder architecture\n",
    "- attention model adds each time-step of the encoder and current time-step of the decoder then activates it with softmax and creates new context-vector\n",
    "- the final output is the weighted sum of context-vector and decoder outputs with softmax activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future of this project\n",
    "- after end of training of all the models the next step is calculating the expected output by using [_beam search_](https://en.wikipedia.org/wiki/Beam_search)\n",
    "- fine-tuning of the BERT model and application of this model on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "______\n",
    "## 1. Architecture, 1. Preprocessing\n",
    "- the first architecture, 3 layers of ```LSTM``` cells encoder, 1 layer of ```LSTM``` cells decoder, with simple dense layer with softmax activation at the top\n",
    "\n",
    "- the first preprocessing : just basic lowering of the text and removing parentheses\n",
    "\n",
    "- the results are unsatisfactory, the neural network couldn't catch the meaning of the input sequence and the summaries are bellow expectations, many times the predicted summary is just ```great product```\n",
    "\n",
    "- the results shown aren't meant to show if the model predicted exactly the same output as the original but to show if model shown some understanding of the review\n",
    "\n",
    "- possible explanation of the results:\n",
    "    - the lack of preprocessing meant that the network needed to learn more complex relationships than possible\n",
    "    - without attention layer the preprocessing deficit became even more visible\n",
    "\n",
    "_example of unsatisfactory output 1:_\n",
    "```\n",
    "Review: not only will these cookies satisfy a sweet tooth attack but they're healthy and will calm nausea and or upset stomach strong ginger flavor not sugary so although adults really like them a lot of kids probably won't be crazy about them\n",
    "\n",
    "Original summary: great cookies for adults\n",
    "\n",
    "Predicted summary:  good but not\n",
    "```\n",
    "\n",
    "_example of network showing at least some understanding 1:_\n",
    "```\n",
    "Review: these cookies have just the right amount of sweetness and a lovely chewy texture the cinnamon balances out the added sugar and the sweetness from the chocolate chips it's great that these contain chocolate chips and not the usual raisins because it's unexpected and it's a bonus for chocoholics looking for something more\n",
    "\n",
    "Original summary: very good cookies\n",
    "\n",
    "Predicted summary:  great snack\n",
    "```\n",
    "\n",
    "_example of network showing at least some understanding 2:_\n",
    "```\n",
    "Review: this is the best hot cocoa i have tried for the keurig rich chocolate flavor i used the 6oz setting and it is not watery if you like hot cocoa and the convenience of a k cup this is the one to get\n",
    "\n",
    "Original summary: awesome\n",
    "\n",
    "Predicted summary:  great flavor\n",
    "```\n",
    "\n",
    "- resulting loss on the train dataset: 2.0597\n",
    "- resulting loss on the validation dataset: 2.3272"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Architecture 1. Preprocessing\n",
    "- 3 layers of ```LSTM``` encoder, 1 layer ```LSTM``` decoder, on the top of encoder-decoder there is attention layer\n",
    "\n",
    "- the first preprocessing : just basic lowering of the text and removing parentheses\n",
    "\n",
    "- following the results of the first architecture, this one succeeded a bit more, there isn't as many meaningless reviews and the network shown some nice understanding of the topic however looking at the same review-summary examples as above, the network tried, but ultimately failed\n",
    "\n",
    "_example of unsatisfactory output 1:_\n",
    "```\n",
    "Review: not only will these cookies satisfy a sweet tooth attack but they're healthy and will calm nausea and or upset stomach strong ginger flavor not sugary so although adults really like them a lot of kids probably won't be crazy about them\n",
    "\n",
    "Original summary: great cookies for adults\n",
    "\n",
    "Predicted summary:  not a fan of\n",
    "```\n",
    "\n",
    "_example of network showing at least some understanding 1:_\n",
    "```\n",
    "Review: these cookies have just the right amount of sweetness and a lovely chewy texture the cinnamon balances out the added sugar and the sweetness from the chocolate chips it's great that these contain chocolate chips and not the usual raisins because it's unexpected and it's a bonus for chocoholics looking for something more\n",
    "\n",
    "Original summary: very good cookies\n",
    "\n",
    "Predicted summary:  great tasting\n",
    "```\n",
    "\n",
    "_example of network showing at least some understanding 2:_\n",
    "```\n",
    "Review: this is the best hot cocoa i have tried for the keurig rich chocolate flavor i used the 6oz setting and it is not watery if you like hot cocoa and the convenience of a k cup this is the one to get \n",
    "\n",
    "Original summary: awesome \n",
    "\n",
    "Predicted summary:  best hot cocoa\n",
    "```\n",
    "\n",
    "- resulting loss on the train dataset: 1.9971\n",
    "- resulting loss on the validation dataset: 2.2523"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architecture 2. Preprocessing\n",
    "- 3 layers of ```LSTM``` encoder, 1 layer ```LSTM``` decoder, on the top simple ```Dense``` layer with ```softmax``` activation\n",
    "\n",
    "- the results were superior to results of 1. architecture without preprocessing, usually the model could remember one or two words from the review and could append some adjective (e.g ```best tea ever```)\n",
    "\n",
    "- however there are still many examples of total misunderstanding of the text\n",
    "\n",
    "_positive example 1:_\n",
    "```\n",
    "Review: tried many peach teas either bitter pale tasting hands favorite peach tea makes amazing iced tea summer robust enough handle served iced sweet enough require minimal sweetners refreshing tea\n",
    "\n",
    "Original summary: best peach tea ever\n",
    "\n",
    "Predicted summary:  best tea ever\n",
    "```\n",
    "\n",
    "_positive example 2:_\n",
    "```\n",
    "Review: bought whim wow consider lucky shot dark turned one best hot sauces ever tasted good spicy heat excellent flavor well unexpected mix heat hint sweet robust mix spices one thing really love flavors whatever put overwhelm heat hot sure linger long flavor original went back amazon bought whole case\n",
    "\n",
    "Original summary: could drink stuff\n",
    "\n",
    "Predicted summary:  best hot sauce\n",
    "```\n",
    "\n",
    "_negative example 1:_\n",
    "```\n",
    "Review: mini schnauzer easily started splintering bone matter minutes ever hungry dog swallowed pieces seeing took bone away ate third less hrs later time breakfast refused eat first time ever refused break overnight fast minutes hunger must gotten better gave ate almost hour later threw four rounds puking could see small softened yet still relatively firm pieces healthy edible bone vomit shame went thinking would great alternative feeding dog indigestible nylon\n",
    "\n",
    "Original summary: made dog throw bad\n",
    "\n",
    "Predicted summary:  dog likes\n",
    "```\n",
    "\n",
    "- resulting loss on the train dataset: 1.7513\n",
    "- resulting loss on the validation dataset: 1.8970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Architecture 2. Preprocessing\n",
    "- 3 layers of ```LSTM``` encoder, 1 layer ```LSTM``` decoder, on the top of encoder-decoder there is attention layer\n",
    "\n",
    "- the expectations were high as the preprocessing was on another level and the difference between no attention and attention models was quite large in mode without preprocessing\n",
    "\n",
    "- the actual results weren't quite as good, the model many times misunderstood the sentiment of the review (review negative summary positive and vice versa)\n",
    "\n",
    "- although the results are better than with previous architectures\n",
    "\n",
    "_positive example 1:_\n",
    "```\n",
    "Review: consider foodie always trying find best everything tried thomas popcorn one oprah favorite things yes good local costco store popcornopolis people store days impressed zebra really great cheddar like none ever tasted wow good says popcorn good back bag explanation two thumbs perfectly sized individually sealed cones eat one cone one sitting stop good limit setter\n",
    "\n",
    "Original summary: best cheddar popcorn ever\n",
    "\n",
    "Predicted summary:  best popcorn\n",
    "```\n",
    "\n",
    "_positive example 2:_\n",
    "```\n",
    "Review: product great taste quite regular milk taste taste like normal powdered milk unique pleasant taste however heavenly unique taste mix glass milk add pappy sassafras add favorite sweetener drank truly great\n",
    "\n",
    "Original summary: tastes great quite milk\n",
    "\n",
    "Predicted summary:  great taste\n",
    "```\n",
    "\n",
    "_negative, rather funny example:_\n",
    "```\n",
    "Review: think reviewers must got bad batch maybe read directions found quite easy prepare topping heated seconds found flavor good compare good chinese take quick lunch time listed grams protein grams fiber sodium somewhat lower competing brands overall best brand found usually add something like fresh apple microwave meal worried environment steamer tray saved reused preparing instant rice meals home alternative meal healthy choice sweet sour chicken also tried\n",
    "\n",
    "Original summary: quick microwave lunch\n",
    "\n",
    "Predicted summary:  tasty healthy\n",
    "```\n",
    "\n",
    "- resulting loss on the train dataset: not collected\n",
    "- resulting loss on the validation dataset: not collected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralNetsTensorflow",
   "language": "python",
   "name": "neuralnetstensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
