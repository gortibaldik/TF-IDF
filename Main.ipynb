{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct comparison of methods of text comprehension\n",
    "- using [__Amazon Fine Food Reviews__](https://www.kaggle.com/snap/amazon-fine-food-reviews) dataset\n",
    "- the dataset is in form of ```.csv``` document, the interesting part being columns _Text_ and _Summary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data: 992\n"
     ]
    }
   ],
   "source": [
    "from main_ntb_backend import dataset, statsKeeper\n",
    "from text_classifiers.text_rank_classifier import keywords_review as textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset._data['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good Quality Dog Food'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset._data['Summary'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the aim is to present different methods of text comprehension, namely:\n",
    "    - ```tf-idf```\n",
    "    - ```textrank```\n",
    "    - abstractive methods based on recurrent neural networks\n",
    "    \n",
    "- the main result is the importance of text preprocessing where ```tf-idf``` fails significantly without correct preprocessing (which is shown in ```TF-IDFNotebook.ipynb```) and RNNs are able to achieve better results with preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency - Inverse Document Frequency\n",
    "- each review is being treated as a separate document\n",
    "- after cruising through all the reviews, the document frequency of a word means how many reviews contain this particular word\n",
    "- term frequency is statistics of a single review, its being calculated as $\\frac{Occurences Of Word}{Number Of Words In The Review}$\n",
    "- inverse document frequency is negative logarithm of document frequency $\\ln(\\frac{Number Of Documents}{Document Frequency Of Word + 1})$\n",
    "\n",
    "- we keep the best 5 words from the review as a measure what is the most important part of the document\n",
    "\n",
    "- all the statistics are kept in ```statsKeeper```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('better', 0.28648286272145923),\n",
       " ('product', 0.24837373829266315),\n",
       " ('vitality', 0.09399881643554253),\n",
       " ('labrador', 0.09399881643554253),\n",
       " ('appreciates', 0.09399881643554253)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statsKeeper._documents[\"0\"].sorted_tf_idfs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textrank algorithm\n",
    "- from the paper [\"TextRank: Bringing Order into Text\"](http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)\n",
    "- the algorithm based on ```PageRank```, ```TextRank``` evaluates the importance of a word in a document by the number of ```links``` to the word from the other parts of document\n",
    "- the algorithm creates a graph of words appearing in a single document and draws an edge between any two words whenever they occur in the same window (meaning they are at most ```N``` words appart)\n",
    "- then the ```PageRank``` algorithm is used on the graph\n",
    "- ```PageRank``` computes the importance of a word as the probability of 'browsing' to current node based on the number of ingoing edges and the importance of their second nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2118     1  several vitality canned dog food products\n",
      "[several vitality canned dog food products]\n",
      "0.1950     1  good quality product\n",
      "[good quality product]\n"
     ]
    }
   ],
   "source": [
    "textrank(dataset._data['cleaned_text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks\n",
    "- 3 different architectures with 2 different modes of text preprocessing are used\n",
    "- the main component of the neural network is an ```LSTM``` cell\n",
    "/n",
    "### 1. Architecture\n",
    "- encoder-decoder architecture of 3 stacked ```LSTM``` nodes in the encoder the last state passed to the only level of ```LSTM``` cells in the decoder with the softmax activation on the outputs of each time step\n",
    "- since this is the simplest architecture, the expectation is that reviews from this architecture will be of the lowest quality\n",
    "\n",
    "### 2. Architecure\n",
    "- encoder-decoder architecture, the same encoder as before, the same decoder as before, with attention on the top of encoder and decoder\n",
    "- good quality reviews are expected from this architecture\n",
    "\n",
    "### 3. Architecture\n",
    "- encoder-decoder architecture, which, in the encoder, uses bidirectional ```LSTM``` cells where the directions are concatenated, the decoder uses unidirectional ```LSTM``` cell, wit attention on the top of encoder and decoder\n",
    "- the training is expected to take more than week (since only CPUs are used) therefore the results (which are expected to be of the best quality) will be added later\n",
    "\n",
    "### 1. Preprocessing\n",
    "- only lowercasing\n",
    "\n",
    "### 2. Preprocessing\n",
    "- lowercasing, deleting links, contraction_mapping, deleting all the special characters and short words\n",
    "\n",
    "- with the second preprocessing the results will hopefully be of better quality, because the neural network will have to learn less difficult connections between the words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralNetsTensorflow",
   "language": "python",
   "name": "neuralnetstensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
