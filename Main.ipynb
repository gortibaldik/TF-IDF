{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct comparison of methods of text comprehension\n",
    "- using [__Amazon Fine Food Reviews__](https://www.kaggle.com/snap/amazon-fine-food-reviews) dataset\n",
    "- the dataset is in form of ```.csv``` document, the interesting part being columns _Text_ and _Summary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data: 992\n"
     ]
    }
   ],
   "source": [
    "from main_ntb_backend import dataset, statsKeeper\n",
    "from text_classifiers.text_rank_classifier import keywords_review as textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset._data['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good Quality Dog Food'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset._data['Summary'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the aim is to present different methods of text comprehension, namely:\n",
    "    - ```tf-idf```\n",
    "    - ```textrank```\n",
    "    - abstractive methods based on recurrent neural networks\n",
    "    \n",
    "- the main result is the importance of text preprocessing where ```tf-idf``` fails significantly without correct preprocessing (which is shown in ```TF-IDFNotebook.ipynb```) and RNNs are able to achieve better results with preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency - Inverse Document Frequency\n",
    "- each review is being treated as a separate document\n",
    "- after cruising through all the reviews, the document frequency of a word means how many reviews contain this particular word\n",
    "- term frequency is statistics of a single review, its being calculated as $\\frac{Occurences Of Word}{Number Of Words In The Review}$\n",
    "- inverse document frequency is negative logarithm of document frequency $\\ln(\\frac{Number Of Documents}{Document Frequency Of Word + 1})$\n",
    "\n",
    "- we keep the best 5 words from the review as a measure what is the most important part of the document\n",
    "\n",
    "- all the statistics are kept in ```statsKeeper```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('better', 0.28648286272145923),\n",
       " ('product', 0.24837373829266315),\n",
       " ('vitality', 0.09399881643554253),\n",
       " ('labrador', 0.09399881643554253),\n",
       " ('appreciates', 0.09399881643554253)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statsKeeper._documents[\"0\"].sorted_tf_idfs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textrank algorithm\n",
    "- from the paper [\"TextRank: Bringing Order into Text\"](http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)\n",
    "- the algorithm based on ```PageRank```, ```TextRank``` evaluates the importance of a word in a document by the number of ```links``` to the word from the other parts of document\n",
    "- the algorithm creates a graph of words appearing in a single document and draws an edge between any two words whenever they occur in the same window (meaning they are at most ```N``` words appart)\n",
    "- then the ```PageRank``` algorithm is used on the graph\n",
    "- ```PageRank``` computes the importance of a word as the probability of 'browsing' to current node based on the number of ingoing edges and the importance of their second nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2118     1  several vitality canned dog food products\n",
      "[several vitality canned dog food products]\n",
      "0.1950     1  good quality product\n",
      "[good quality product]\n"
     ]
    }
   ],
   "source": [
    "textrank(dataset._data['cleaned_text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks\n",
    "- 3 different architectures with 2 different modes of text preprocessing are used\n",
    "- the main component of the neural network is an ```LSTM``` cell\n",
    "\n",
    "### 1. Architecture\n",
    "- encoder-decoder architecture of 3 stacked ```LSTM``` nodes in the encoder the last state passed to the only level of ```LSTM``` cells in the decoder with the softmax activation on the outputs of each time step\n",
    "- since this is the simplest architecture, the expectation is that reviews from this architecture will be of the lowest quality\n",
    "\n",
    "### 2. Architecure\n",
    "- encoder-decoder architecture, the same encoder as before, the same decoder as before, with attention on the top of encoder and decoder\n",
    "- good quality reviews are expected from this architecture\n",
    "\n",
    "### 3. Architecture\n",
    "- encoder-decoder architecture, which, in the encoder, uses bidirectional ```LSTM``` cells where the directions are concatenated, the decoder uses unidirectional ```LSTM``` cell, wit attention on the top of encoder and decoder\n",
    "- the training is expected to take more than week (since only CPUs are used) therefore the results (which are expected to be of the best quality) will be added later\n",
    "\n",
    "### 1. Preprocessing\n",
    "- only lowercasing\n",
    "\n",
    "### 2. Preprocessing\n",
    "- lowercasing, deleting links, contraction_mapping, deleting all the special characters and short words\n",
    "\n",
    "- with the second preprocessing the results will hopefully be of better quality, because the neural network will have to learn less difficult connections between the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention mechanism\n",
    "- based on [paper](https://arxiv.org/abs/1409.0473)\n",
    "- in standard encoder-decoder architecture the encoder learns the context of the input sequence and returns it as a single fixed-size vector, therefore decoder needs to learn to translate fixed-size vector into human-readable output\n",
    "- in attention driven encoder-decoder the encoder outputs the learned context at each time-step, then the final human-readable output isn't pure output of the decoder but output of the attention model at the top of encoder-decoder architecture\n",
    "- attention model adds each time-step of the encoder and current time-step of the decoder then activates it with softmax and creates new context-vector\n",
    "- the final output is the weighted sum of context-vector and decoder outputs with softmax activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future of this project\n",
    "- after end of training of all the models the next step is calculating the expected output by using [_beam search_](https://en.wikipedia.org/wiki/Beam_search)\n",
    "- fine-tuning of the BERT model and application of this model on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "______\n",
    "## 1. Architecture, 1. Preprocessing\n",
    "- the first architecture, 3 layers of ```LSTM``` cells encoder, 1 layer of ```LSTM``` cells decoder, with simple dense layer with softmax activation at the top\n",
    "\n",
    "- the first preprocessing : just basic lowering of the text and removing parentheses\n",
    "\n",
    "- the results are unsatisfactory, the neural network couldn't catch the meaning of the input sequence and the summaries are bellow expectations, many times the predicted summary is just ```great product``` even if the review has really negative conotations\n",
    "\n",
    "- possible explanation of the results:\n",
    "    - the lack of preprocessing meant that the network needed to learn more complex relationships than possible\n",
    "    - without attention layer the preprocessing deficit became even more visible\n",
    "\n",
    "_example of unsatisfactory output:_\n",
    "```\n",
    "Review: not only will these cookies satisfy a sweet tooth attack but they're healthy and will calm nausea and or upset stomach strong ginger flavor not sugary so although adults really like them a lot of kids probably won't be crazy about them\n",
    "\n",
    "Original summary: great cookies for adults\n",
    "\n",
    "Predicted summary:  good but not\n",
    "```\n",
    "\n",
    "_example of network showing at least some understanding:_\n",
    "```\n",
    "Review: these cookies have just the right amount of sweetness and a lovely chewy texture the cinnamon balances out the added sugar and the sweetness from the chocolate chips it's great that these contain chocolate chips and not the usual raisins because it's unexpected and it's a bonus for chocoholics looking for something more\n",
    "\n",
    "Original summary: very good cookies\n",
    "\n",
    "Predicted summary:  great snack\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralNetsTensorflow",
   "language": "python",
   "name": "neuralnetstensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
